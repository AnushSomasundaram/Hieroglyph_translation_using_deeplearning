{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import PIL \n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "def process_and_save_images(input_path, output_folder):\n",
    "    # Load image, grayscale, Gaussian blur, Otsu's threshold, dilate\n",
    "    image = cv2.imread(input_path)\n",
    "    original = image.copy()\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    blur = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    thresh = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (15, 15))\n",
    "    dilate = cv2.dilate(thresh, kernel, iterations=2)\n",
    "\n",
    "    # Find contours, obtain bounding box coordinates, and extract ROI\n",
    "    cnts = cv2.findContours(dilate, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
    "    \n",
    "    # Ensure the output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    image_number = 0\n",
    "    for c in cnts:\n",
    "        x, y, w, h = cv2.boundingRect(c)\n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), (36, 255, 12), 3)\n",
    "        ROI = original[y:y + h, x:x + w]\n",
    "        output_path = os.path.join(output_folder, \"ROI_{}.png\".format(image_number))\n",
    "        cv2.imwrite(output_path, ROI)\n",
    "        image_number += 1\n",
    "\n",
    "#############################################################################################################################################################################################################\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "#############################################################################################################################################################################################################\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import cv2\n",
    "# Define the path to your dataset folder\n",
    "data_path = r\"/Users/software/Downloads/hieroglyphics_nlp/Code_image_augmentation/augmented_images_dataset\"\n",
    "\n",
    "# Define the transformation to apply to the images (e.g., resizing, normalization)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize if needed\n",
    "])\n",
    "\n",
    "# Create an instance of ImageFolder and apply the transformation\n",
    "dataset = (ImageFolder(root=data_path, transform=transform))\n",
    "\n",
    "# Create a data loader to load the images in batches during training or evaluation\n",
    "batch_size = 32\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import vgg16\n",
    "import torchvision.models as models\n",
    "\n",
    "def predict_vgg16_single(image_path,model_path):\n",
    "    # load custom vgg model trained on azhars pc\n",
    "    \n",
    "    model = models.vgg16(pretrained=False, num_classes=1000)\n",
    "    model.classifier[6] = nn.Linear(4096, 673)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    \n",
    "    \n",
    "    #preprocessing the image\n",
    "    \n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    input_image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "    input_data = preprocess(input_image)\n",
    "    input_data = input_data.unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_data)\n",
    "        \n",
    "    #Get the predicted class index\n",
    "    \n",
    "    _ , predicted_idx = torch.max(outputs,1)\n",
    "    predicted_index = predicted_idx.item()\n",
    "    \n",
    "    return predicted_index\n",
    "\n",
    "\n",
    "#############################################################################################################################################################################################################\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "#############################################################################################################################################################################################################\n",
    "\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "# !pip install keytotext \n",
    "# keyword to sentence generator\n",
    "\n",
    "from keytotext import pipeline\n",
    "nlp = pipeline(\"k2t\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'copy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m input_image_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/Users/software/Downloads/hieroglyphics_nlp/pipeline/Screenshot 2023-11-18 at 11.15.27 PM.png\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      4\u001b[0m output_folder_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/Users/software/Downloads/hieroglyphics_nlp/pipeline/extracted_images\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 5\u001b[0m process_and_save_images(input_image_path, output_folder_path)\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mIPython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdisplay\u001b[39;00m \u001b[39mimport\u001b[39;00m Image\n\u001b[1;32m      9\u001b[0m Image (input_image_path)\n",
      "Cell \u001b[0;32mIn[7], line 11\u001b[0m, in \u001b[0;36mprocess_and_save_images\u001b[0;34m(input_path, output_folder)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess_and_save_images\u001b[39m(input_path, output_folder):\n\u001b[1;32m      9\u001b[0m     \u001b[39m# Load image, grayscale, Gaussian blur, Otsu's threshold, dilate\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     image \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mimread(input_path)\n\u001b[0;32m---> 11\u001b[0m     original \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39;49mcopy()\n\u001b[1;32m     12\u001b[0m     gray \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mcvtColor(image, cv2\u001b[39m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[1;32m     13\u001b[0m     blur \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mGaussianBlur(gray, (\u001b[39m5\u001b[39m, \u001b[39m5\u001b[39m), \u001b[39m0\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'copy'"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "\n",
    "input_image_path = '/Users/software/Downloads/hieroglyphics_nlp/pipeline/Screenshot 2023-11-18 at 11.15.27 PM.png'\n",
    "output_folder_path = '/Users/software/Downloads/hieroglyphics_nlp/pipeline/extracted_images'\n",
    "process_and_save_images(input_image_path, output_folder_path)\n",
    "\n",
    "\n",
    "from IPython.display import Image\n",
    "Image (input_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "process_images_in_folder() missing 1 required positional argument: 'dictionary'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 20\u001b[0m\n\u001b[1;32m     16\u001b[0m         hieroglyph_unicodes\u001b[39m.\u001b[39mappend(dictionary[predicted_index])\n\u001b[1;32m     17\u001b[0m     \u001b[39mreturn\u001b[39;00m hieroglyph_unicodes\n\u001b[0;32m---> 20\u001b[0m required_hieroglyphs \u001b[39m=\u001b[39m process_images_in_folder(\u001b[39m\"\u001b[39;49m\u001b[39m/Users/software/Downloads/hieroglyphics_nlp/pipeline/extracted_images\u001b[39;49m\u001b[39m\"\u001b[39;49m, model_path)\n\u001b[1;32m     22\u001b[0m \u001b[39mprint\u001b[39m(required_hieroglyphs)\n",
      "\u001b[0;31mTypeError\u001b[0m: process_images_in_folder() missing 1 required positional argument: 'dictionary'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "model_path = '/Users/software/Downloads/hieroglyphics_nlp/hieroglyph recognition/vgg-16/hieroglyph_vgg_16-mps.pth'\n",
    "\n",
    "def process_images_in_folder(input_folder, model_path):\n",
    "    # Loop through all files in the input folder\n",
    "    hieroglyph_unicodes = []\n",
    "    for filename in os.listdir(input_folder):\n",
    "        # Construct the full path for the input image\n",
    "        image_path = os.path.join(input_folder, filename)\n",
    "\n",
    "        # Predict and print the result\n",
    "        predicted_index = predict_vgg16_single(image_path, model_path)\n",
    "        hieroglyph_unicodes.append(dataset.classes[predicted_index])\n",
    "    return hieroglyph_unicodes\n",
    "\n",
    "\n",
    "required_hieroglyphs = process_images_in_folder(\"/Users/software/Downloads/hieroglyphics_nlp/pipeline/extracted_images\", model_path)\n",
    "\n",
    "print(required_hieroglyphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input to heiroglyph translator model: Translate hieroglyph unicode sequence to English: N12 C10 S34 K1\n",
      "Model Output (keywords): a man\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hieroglyphs_unicodes = \" \".join([str(item) for item in required_hieroglyphs])\n",
    "text = \"Translate hieroglyph unicode sequence to English: \"+ hieroglyphs_unicodes\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"AnushS/hieroglyph_unicode_translator_t5_small\")\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"AnushS/hieroglyph_unicode_translator_t5_small\")\n",
    "outputs = model.generate(inputs, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)\n",
    "translated_keywords = str(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "\n",
    "\n",
    "print(\"Input to heiroglyph translator model: \" + text)\n",
    "print(\"Model Output (keywords): \" + translated_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
