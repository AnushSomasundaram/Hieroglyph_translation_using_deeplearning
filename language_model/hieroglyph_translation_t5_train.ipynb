{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33ad8551",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbe20deae0cd4d96b423fd20bc85a03e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#login to hugging face\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "341abdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"/Users/software/Downloads/hieroglyphics_nlp/language_model/hieroglyph_translation_t5_train.ipynb\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf9c448b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import ast\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20880973",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/software/Downloads/hieroglyphics_nlp/language_model/hieroglyph_translation_dictionary_storage.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4164b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72238295",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_dict(string):\n",
    "    try:\n",
    "        string = string.replace(\"'\", '\"')\n",
    "        return json.loads(string)\n",
    "    except json.JSONDecodeError as e:\n",
    "        \n",
    "        return None\n",
    "df.drop(\"Unnamed: 0\",axis=1)\n",
    "df['Translations'] = df['Translations'].apply(convert_to_dict)\n",
    "df.to_csv(\"hello.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "092b7586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Translations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'se': 'A1', 'de': 'I, me, my '}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'se': 'A1 A1 A1', 'de': 'man, men, mankind, E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'se': 'A2', 'de': 'drink '}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'se': 'A6', 'de': 'pure, purify oneself, bath...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'se': 'A6A', 'de': 'pure, purify oneself, bat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11718</th>\n",
       "      <td>{'se': 'Aa32 X1 G4 T14 A1 N33A', 'de': 'Nubian...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11719</th>\n",
       "      <td>{'se': 'Aa32 X1 N25', 'de': 'Nubia '}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11720</th>\n",
       "      <td>{'se': 'Aa32 X1 N25 T14 A1 N33A', 'de': 'Nubia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11721</th>\n",
       "      <td>{'se': 'Aa32 X1 Z4 N33 Z2', 'de': 'Nubian mine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11722</th>\n",
       "      <td>{'se': 'Aa32 X1 Z8', 'de': 'Nubia '}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11723 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Translations\n",
       "0                       {'se': 'A1', 'de': 'I, me, my '}\n",
       "1      {'se': 'A1 A1 A1', 'de': 'man, men, mankind, E...\n",
       "2                           {'se': 'A2', 'de': 'drink '}\n",
       "3      {'se': 'A6', 'de': 'pure, purify oneself, bath...\n",
       "4      {'se': 'A6A', 'de': 'pure, purify oneself, bat...\n",
       "...                                                  ...\n",
       "11718  {'se': 'Aa32 X1 G4 T14 A1 N33A', 'de': 'Nubian...\n",
       "11719              {'se': 'Aa32 X1 N25', 'de': 'Nubia '}\n",
       "11720  {'se': 'Aa32 X1 N25 T14 A1 N33A', 'de': 'Nubia...\n",
       "11721  {'se': 'Aa32 X1 Z4 N33 Z2', 'de': 'Nubian mine...\n",
       "11722               {'se': 'Aa32 X1 Z8', 'de': 'Nubia '}\n",
       "\n",
       "[11723 rows x 1 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(\"Unnamed: 0\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bc30259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                         {'se': 'A1', 'de': 'I, me, my '}\n",
       "1        {'se': 'A1 A1 A1', 'de': 'man, men, mankind, E...\n",
       "2                             {'se': 'A2', 'de': 'drink '}\n",
       "3        {'se': 'A6', 'de': 'pure, purify oneself, bath...\n",
       "4        {'se': 'A6A', 'de': 'pure, purify oneself, bat...\n",
       "                               ...                        \n",
       "11718    {'se': 'Aa32 X1 G4 T14 A1 N33A', 'de': 'Nubian...\n",
       "11719                {'se': 'Aa32 X1 N25', 'de': 'Nubia '}\n",
       "11720    {'se': 'Aa32 X1 N25 T14 A1 N33A', 'de': 'Nubia...\n",
       "11721    {'se': 'Aa32 X1 Z4 N33 Z2', 'de': 'Nubian mine...\n",
       "11722                 {'se': 'Aa32 X1 Z8', 'de': 'Nubia '}\n",
       "Name: Translations, Length: 11723, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df[\"Translations\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19fe83d7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['Unnamed: 0', 'Translations'],\n",
      "    num_rows: 11723\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# lets get our custom dataset in order\n",
    "\n",
    "# Replace 'your_dataset.csv' with the actual file path of your CSV file\n",
    "file_path = 'hieroglyph_translation_dictionary_storage_final.csv'\n",
    "\n",
    "# Load your dataset into a Dataset object\n",
    "hieroglyph_translation = Dataset.from_pandas(df)\n",
    "\n",
    "\n",
    "# Print a sample from your dataset\n",
    "print(hieroglyph_translation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48c46ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11000, 2)\n",
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "df1 = df[0:11000]\n",
    "df2 = df[10723:11724]\n",
    "print(df1.shape)\n",
    "print(df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65f27626",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset.from_pandas(df1, split=\"train\")\n",
    "test_ds = Dataset.from_pandas(df2, split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2385385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'de': 'man, men, mankind, Egyptians ', 'se': 'A1 A1 A1'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_ds[1]['Translations'])\n",
    "type(train_ds[1]['Translations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d460396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "type(hieroglyph_translation[\"Translations\"][1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17da1c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ce6d0e9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "814a54451d994568b54aa79aae8537f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aa7e108d5f244298ae77508da9292d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "source_lang = \"se\"\n",
    "target_lang = \"de\"\n",
    "prefix = \"Translate hieroglyph unicode sequence to English: \"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    #try:\n",
    "    inputs=[]\n",
    "    targets=[]\n",
    "    x=0\n",
    "    for example in examples['Translations']:\n",
    "        \n",
    "        try:\n",
    "            inputs.append(prefix+example[source_lang])\n",
    "            targets.append(example[target_lang])\n",
    "        except:\n",
    "            inputs.append(\"None\")\n",
    "            targets.append(\"None\")\n",
    "            \n",
    "        \n",
    "\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=1000, truncation=True)\n",
    "    return model_inputs\n",
    "\n",
    "    #except:\n",
    "    #    print(\"Aiyayo Vada Poche\")\n",
    "    \n",
    "tokenized_train_hieroglyph_translations = train_ds.map(preprocess_function, batched=True)\n",
    "\n",
    "tokenized_test_hieroglyph_translations = test_ds.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c6cc751",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': 10723,\n",
       " 'Translations': {'de': 'my ', 'se': 'X1 G1 M17 M17 A1'},\n",
       " 'input_ids': [30355,\n",
       "  15,\n",
       "  1382,\n",
       "  32,\n",
       "  29929,\n",
       "  73,\n",
       "  23,\n",
       "  4978,\n",
       "  5932,\n",
       "  12,\n",
       "  1566,\n",
       "  10,\n",
       "  3,\n",
       "  4,\n",
       "  536,\n",
       "  350,\n",
       "  536,\n",
       "  283,\n",
       "  2517,\n",
       "  283,\n",
       "  2517,\n",
       "  71,\n",
       "  536,\n",
       "  1],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': [82, 1]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_test_hieroglyph_translations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8242cc05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': 2,\n",
       " 'Translations': {'de': 'drink ', 'se': 'A2'},\n",
       " 'input_ids': [30355,\n",
       "  15,\n",
       "  1382,\n",
       "  32,\n",
       "  29929,\n",
       "  73,\n",
       "  23,\n",
       "  4978,\n",
       "  5932,\n",
       "  12,\n",
       "  1566,\n",
       "  10,\n",
       "  71,\n",
       "  357,\n",
       "  1],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " 'labels': [3281, 1]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_hieroglyph_translations[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fefc762",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "427fbae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "69880eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "714d3471",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad2bd92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e41baeca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/AnushS/hieroglyph_10_epochs into local empty directory.\n",
      "/Users/software/anaconda3/envs/mlp/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/software/anaconda3/envs/mlp/lib/python3.8/site-packages/wandb/sdk/wandb_init.py\", line 1162, in init\n",
      "    wi.setup(kwargs)\n",
      "  File \"/Users/software/anaconda3/envs/mlp/lib/python3.8/site-packages/wandb/sdk/wandb_init.py\", line 306, in setup\n",
      "    wandb_login._login(\n",
      "  File \"/Users/software/anaconda3/envs/mlp/lib/python3.8/site-packages/wandb/sdk/wandb_login.py\", line 298, in _login\n",
      "    wlogin.prompt_api_key()\n",
      "  File \"/Users/software/anaconda3/envs/mlp/lib/python3.8/site-packages/wandb/sdk/wandb_login.py\", line 221, in prompt_api_key\n",
      "    key, status = self._prompt_api_key()\n",
      "  File \"/Users/software/anaconda3/envs/mlp/lib/python3.8/site-packages/wandb/sdk/wandb_login.py\", line 201, in _prompt_api_key\n",
      "    key = apikey.prompt_api_key(\n",
      "  File \"/Users/software/anaconda3/envs/mlp/lib/python3.8/site-packages/wandb/sdk/lib/apikey.py\", line 144, in prompt_api_key\n",
      "    key = input_callback(api_ask).strip()\n",
      "  File \"/Users/software/anaconda3/envs/mlp/lib/python3.8/site-packages/click/termui.py\", line 164, in prompt\n",
      "    value = prompt_func(prompt)\n",
      "  File \"/Users/software/anaconda3/envs/mlp/lib/python3.8/site-packages/click/termui.py\", line 147, in prompt_func\n",
      "    raise Abort() from None\n",
      "click.exceptions.Abort\n"
     ]
    },
    {
     "ename": "Error",
     "evalue": "An unexpected error occurred",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAbort\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/mlp/lib/python3.8/site-packages/wandb/sdk/wandb_init.py:1162\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[1;32m   1161\u001b[0m wi \u001b[39m=\u001b[39m _WandbInit()\n\u001b[0;32m-> 1162\u001b[0m wi\u001b[39m.\u001b[39;49msetup(kwargs)\n\u001b[1;32m   1163\u001b[0m \u001b[39massert\u001b[39;00m wi\u001b[39m.\u001b[39msettings\n",
      "File \u001b[0;32m~/anaconda3/envs/mlp/lib/python3.8/site-packages/wandb/sdk/wandb_init.py:306\u001b[0m, in \u001b[0;36m_WandbInit.setup\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m settings\u001b[39m.\u001b[39m_offline \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m settings\u001b[39m.\u001b[39m_noop:\n\u001b[0;32m--> 306\u001b[0m     wandb_login\u001b[39m.\u001b[39;49m_login(\n\u001b[1;32m    307\u001b[0m         anonymous\u001b[39m=\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mpop(\u001b[39m\"\u001b[39;49m\u001b[39manonymous\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    308\u001b[0m         force\u001b[39m=\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mpop(\u001b[39m\"\u001b[39;49m\u001b[39mforce\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    309\u001b[0m         _disable_warning\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    310\u001b[0m         _silent\u001b[39m=\u001b[39;49msettings\u001b[39m.\u001b[39;49mquiet \u001b[39mor\u001b[39;49;00m settings\u001b[39m.\u001b[39;49msilent,\n\u001b[1;32m    311\u001b[0m         _entity\u001b[39m=\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mentity\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mor\u001b[39;49;00m settings\u001b[39m.\u001b[39;49mentity,\n\u001b[1;32m    312\u001b[0m     )\n\u001b[1;32m    314\u001b[0m \u001b[39m# apply updated global state after login was handled\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mlp/lib/python3.8/site-packages/wandb/sdk/wandb_login.py:298\u001b[0m, in \u001b[0;36m_login\u001b[0;34m(anonymous, key, relogin, host, force, timeout, _backend, _silent, _disable_warning, _entity)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m key:\n\u001b[0;32m--> 298\u001b[0m     wlogin\u001b[39m.\u001b[39;49mprompt_api_key()\n\u001b[1;32m    300\u001b[0m \u001b[39m# make sure login credentials get to the backend\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mlp/lib/python3.8/site-packages/wandb/sdk/wandb_login.py:221\u001b[0m, in \u001b[0;36m_WandbLogin.prompt_api_key\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprompt_api_key\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 221\u001b[0m     key, status \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prompt_api_key()\n\u001b[1;32m    222\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m==\u001b[39m ApiKeyStatus\u001b[39m.\u001b[39mNOTTY:\n",
      "File \u001b[0;32m~/anaconda3/envs/mlp/lib/python3.8/site-packages/wandb/sdk/wandb_login.py:201\u001b[0m, in \u001b[0;36m_WandbLogin._prompt_api_key\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     key \u001b[39m=\u001b[39m apikey\u001b[39m.\u001b[39;49mprompt_api_key(\n\u001b[1;32m    202\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_settings,\n\u001b[1;32m    203\u001b[0m         api\u001b[39m=\u001b[39;49mapi,\n\u001b[1;32m    204\u001b[0m         no_offline\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_settings\u001b[39m.\u001b[39;49mforce \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_settings \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    205\u001b[0m         no_create\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_settings\u001b[39m.\u001b[39;49mforce \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_settings \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    206\u001b[0m     )\n\u001b[1;32m    207\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    208\u001b[0m     \u001b[39m# invalid key provided, try again\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mlp/lib/python3.8/site-packages/wandb/sdk/lib/apikey.py:144\u001b[0m, in \u001b[0;36mprompt_api_key\u001b[0;34m(settings, api, input_callback, browser_callback, no_offline, no_create, local)\u001b[0m\n\u001b[1;32m    141\u001b[0m     wandb\u001b[39m.\u001b[39mtermlog(\n\u001b[1;32m    142\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYou can find your API key in your browser here: \u001b[39m\u001b[39m{\u001b[39;00mapp_url\u001b[39m}\u001b[39;00m\u001b[39m/authorize\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    143\u001b[0m     )\n\u001b[0;32m--> 144\u001b[0m     key \u001b[39m=\u001b[39m input_callback(api_ask)\u001b[39m.\u001b[39mstrip()\n\u001b[1;32m    145\u001b[0m write_key(settings, key, api\u001b[39m=\u001b[39mapi)\n",
      "File \u001b[0;32m~/anaconda3/envs/mlp/lib/python3.8/site-packages/click/termui.py:164\u001b[0m, in \u001b[0;36mprompt\u001b[0;34m(text, default, hide_input, confirmation_prompt, type, value_proc, prompt_suffix, show_default, err, show_choices)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     value \u001b[39m=\u001b[39m prompt_func(prompt)\n\u001b[1;32m    165\u001b[0m     \u001b[39mif\u001b[39;00m value:\n",
      "File \u001b[0;32m~/anaconda3/envs/mlp/lib/python3.8/site-packages/click/termui.py:147\u001b[0m, in \u001b[0;36mprompt.<locals>.prompt_func\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    146\u001b[0m     echo(\u001b[39mNone\u001b[39;00m, err\u001b[39m=\u001b[39merr)\n\u001b[0;32m--> 147\u001b[0m \u001b[39mraise\u001b[39;00m Abort() \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mAbort\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 25\u001b[0m\n\u001b[1;32m      1\u001b[0m training_args \u001b[39m=\u001b[39m Seq2SeqTrainingArguments(\n\u001b[1;32m      2\u001b[0m     output_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhieroglyph_10_epochs\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     evaluation_strategy\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     push_to_hub\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m trainer \u001b[39m=\u001b[39m Seq2SeqTrainer(\n\u001b[1;32m     16\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m     17\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     compute_metrics\u001b[39m=\u001b[39mcompute_metrics,\n\u001b[1;32m     23\u001b[0m )\n\u001b[0;32m---> 25\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/anaconda3/envs/mlp/lib/python3.8/site-packages/transformers/trainer.py:1633\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1628\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1630\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1631\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1632\u001b[0m )\n\u001b[0;32m-> 1633\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1634\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1635\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1636\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1637\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1638\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/mlp/lib/python3.8/site-packages/transformers/trainer.py:1818\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1815\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step\n\u001b[1;32m   1816\u001b[0m model\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m-> 1818\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcallback_handler\u001b[39m.\u001b[39;49mon_train_begin(args, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstate, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcontrol)\n\u001b[1;32m   1820\u001b[0m \u001b[39m# Skip the first epochs_trained epochs to get the random state of the dataloader at the right point.\u001b[39;00m\n\u001b[1;32m   1821\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m args\u001b[39m.\u001b[39mignore_data_skip:\n",
      "File \u001b[0;32m~/anaconda3/envs/mlp/lib/python3.8/site-packages/transformers/trainer_callback.py:353\u001b[0m, in \u001b[0;36mCallbackHandler.on_train_begin\u001b[0;34m(self, args, state, control)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_train_begin\u001b[39m(\u001b[39mself\u001b[39m, args: TrainingArguments, state: TrainerState, control: TrainerControl):\n\u001b[1;32m    352\u001b[0m     control\u001b[39m.\u001b[39mshould_training_stop \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall_event(\u001b[39m\"\u001b[39;49m\u001b[39mon_train_begin\u001b[39;49m\u001b[39m\"\u001b[39;49m, args, state, control)\n",
      "File \u001b[0;32m~/anaconda3/envs/mlp/lib/python3.8/site-packages/transformers/trainer_callback.py:397\u001b[0m, in \u001b[0;36mCallbackHandler.call_event\u001b[0;34m(self, event, args, state, control, **kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall_event\u001b[39m(\u001b[39mself\u001b[39m, event, args, state, control, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    396\u001b[0m     \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[0;32m--> 397\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(callback, event)(\n\u001b[1;32m    398\u001b[0m             args,\n\u001b[1;32m    399\u001b[0m             state,\n\u001b[1;32m    400\u001b[0m             control,\n\u001b[1;32m    401\u001b[0m             model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel,\n\u001b[1;32m    402\u001b[0m             tokenizer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer,\n\u001b[1;32m    403\u001b[0m             optimizer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer,\n\u001b[1;32m    404\u001b[0m             lr_scheduler\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlr_scheduler,\n\u001b[1;32m    405\u001b[0m             train_dataloader\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_dataloader,\n\u001b[1;32m    406\u001b[0m             eval_dataloader\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meval_dataloader,\n\u001b[1;32m    407\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    408\u001b[0m         )\n\u001b[1;32m    409\u001b[0m         \u001b[39m# A Callback can skip the return of `control` if it doesn't change it.\u001b[39;00m\n\u001b[1;32m    410\u001b[0m         \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/mlp/lib/python3.8/site-packages/transformers/integrations.py:753\u001b[0m, in \u001b[0;36mWandbCallback.on_train_begin\u001b[0;34m(self, args, state, control, model, **kwargs)\u001b[0m\n\u001b[1;32m    751\u001b[0m     args\u001b[39m.\u001b[39mrun_name \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    752\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initialized:\n\u001b[0;32m--> 753\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msetup(args, state, model, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/mlp/lib/python3.8/site-packages/transformers/integrations.py:727\u001b[0m, in \u001b[0;36mWandbCallback.setup\u001b[0;34m(self, args, state, model, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m         init_args[\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m args\u001b[39m.\u001b[39mrun_name\n\u001b[1;32m    726\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wandb\u001b[39m.\u001b[39mrun \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wandb\u001b[39m.\u001b[39;49minit(\n\u001b[1;32m    728\u001b[0m         project\u001b[39m=\u001b[39;49mos\u001b[39m.\u001b[39;49mgetenv(\u001b[39m\"\u001b[39;49m\u001b[39mWANDB_PROJECT\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mhuggingface\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    729\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minit_args,\n\u001b[1;32m    730\u001b[0m     )\n\u001b[1;32m    731\u001b[0m \u001b[39m# add config parameters (run may have been created manually)\u001b[39;00m\n\u001b[1;32m    732\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wandb\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mupdate(combined_dict, allow_val_change\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/mlp/lib/python3.8/site-packages/wandb/sdk/wandb_init.py:1204\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[1;32m   1202\u001b[0m             wandb\u001b[39m.\u001b[39mtermerror(\u001b[39m\"\u001b[39m\u001b[39mAbnormal program exit\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1203\u001b[0m             os\u001b[39m.\u001b[39m_exit(\u001b[39m1\u001b[39m)\n\u001b[0;32m-> 1204\u001b[0m         \u001b[39mraise\u001b[39;00m Error(\u001b[39m\"\u001b[39m\u001b[39mAn unexpected error occurred\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39merror_seen\u001b[39;00m\n\u001b[1;32m   1205\u001b[0m \u001b[39mreturn\u001b[39;00m run\n",
      "\u001b[0;31mError\u001b[0m: An unexpected error occurred"
     ]
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"hieroglyph_10_epochs\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=10,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591eb874",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07d413b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaeb6d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"AnushS/hieroglyph_unicode_translator_t5_small\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"AnushS/hieroglyph_unicode_translator_t5_small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ab06932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the body (of all)\n"
     ]
    }
   ],
   "source": [
    "text = \"Translate hieroglyph unicode sequence to English:  {hieroglyphs}\"\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"AnushS/hieroglyph_unicode_translator_t5_small\")\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"AnushS/hieroglyph_unicode_translator_t5_small\")\n",
    "outputs = model.generate(inputs, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)\n",
    "translated_keywords = str(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "\n",
    "print(translated_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7762ddfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
